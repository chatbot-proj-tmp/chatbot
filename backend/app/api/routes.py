import asyncio
import os
from openai import OpenAI
from fastapi import APIRouter, HTTPException 
from pydantic import BaseModel
from dotenv import load_dotenv  
from fastapi.responses import StreamingResponse
from ..chatbotDirectory import chatbot
from ..chatbotDirectory.functioncalling import tools ,FunctionCalling
from ..chatbotDirectory.functioncalling import model
from ..chatbotDirectory.chatbot import ChatbotStream
import json


# UserRequest í´ë˜ìŠ¤ì— language í•„ë“œ ì¶”ê°€
class UserRequest(BaseModel):
    message: str
    language: str = "KOR"  # ê¸°ë³¸ê°’ì€ í•œêµ­ì–´ë¡œ ì„¤ì •

func_calling = FunctionCalling(
    model=model.basic,
    available_functions={
        # í•„ìš”ì‹œ ë‹¤ë¥¸ í•¨ìˆ˜ë„ ì—¬ê¸°ì— ì¶”ê°€
    }
)
router = APIRouter()

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=api_key)

# ì˜ˆì‹œ: ì „ì—­ ë˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ ë‚´ë¶€ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chatbot = ChatbotStream(
    model=model.advanced,
    system_role="ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ ì±—ë´‡ì…ë‹ˆë‹¤.",
    instruction="ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.",
    user="ëŒ€ê¸°",
    assistant="memmo"
)
    

@router.get("/chat")
async def chat_get():
    return {"message": "Use POST method for chat"}

@router.post("/chat")
async def stream_chat(user_input: UserRequest):
    # 1) ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ìš°ì„  ì›ë³¸ ë¬¸ë§¥ì— ì¶”ê°€
    chatbot.add_user_message_in_context(user_input.message)
    
    # ì–¸ì–´ë³„ ì§€ì¹¨ ë§¤í•‘ ì •ì˜
    instruction_map = {
        "KOR": "í•œêµ­ì–´ë¡œ ì •ì¤‘í•˜ê³  ë”°ëœ»í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”.",
        "ENG": "Please respond kindly in English.",
        "VI": "Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch nháº¹ nhÃ ng.",
        "CN": "è¯·ç”¨ä¸­æ–‡äº²åˆ‡åœ°å›ç­”ã€‚"
        # í•„ìš”ì— ë”°ë¼ ë” ë§ì€ ì–¸ì–´ ì¶”ê°€
    }
    
    # ì‚¬ìš©ìê°€ ì„ íƒí•œ ì–¸ì–´ì— ë”°ë¼ ì§€ì¹¨ ì„ íƒ (ì—†ìœ¼ë©´ í•œêµ­ì–´ ê¸°ë³¸ê°’)
    instruction = instruction_map.get(user_input.language, instruction_map["KOR"])
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ì— ì§€ì¹¨ ì¶”ê°€
    chatbot.context[-1]['content'] += " " + instruction

    analyzed= func_calling.analyze(user_input.message, tools)

    temp_context = chatbot.to_openai_context(context=chatbot.context[:])
    

    for tool_call in analyzed:  # analyzedëŠ” list of function_call dicts
            if tool_call.type != "function_call":
                continue
            func_name = tool_call.name
            func_args = json.loads(tool_call.arguments)
            call_id = tool_call.call_id

            func_to_call = func_calling.available_functions.get(func_name)
            if not func_to_call:
                print(f"[ì˜¤ë¥˜] ë“±ë¡ë˜ì§€ ì•Šì€ í•¨ìˆ˜: {func_name}")
                continue

            try:
               
                function_call_msg = {
                    "type": "function_call",  # ê³ ì •
                    "call_id": call_id,  # ë”•ì…”ë„ˆë¦¬ ë‚´ì— ìˆê±°ë‚˜ keyê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë‹ˆ ì£¼ì˜
                    "name": func_name,
                    "arguments": tool_call.arguments  # dict -> JSON string
                }
                print(f"í•¨ìˆ˜ í˜¸ì¶œ ë©”ì‹œì§€: {function_call_msg}")
                if func_name == "search_internet":
                    # contextëŠ” ì´ë¯¸ run ë©”ì„œë“œì˜ ë§¤ê°œë³€ìˆ˜ë¡œ ë°›ê³  ìˆìŒ
                   func_response = func_to_call(chat_context=chatbot.context[:], **func_args)
                else:
                   func_response = func_to_call(**func_args)

                temp_context.extend([
                    function_call_msg,
                {
                    "type": "function_call_output",
                    "call_id": call_id,
                    "output": str(func_response)
                }
            ])
                print(temp_context)

            except Exception as e:
                print(f"[í•¨ìˆ˜ ì‹¤í–‰ ì˜¤ë¥˜] {func_name}: {e}")

    # 4) í•¨ìˆ˜ í˜¸ì¶œ ê²°ê³¼ê°€ ë°˜ì˜ëœ temp_contextìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìƒì„±
    async def generate_with_tool():
        try:
            # stream=Trueë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            stream = client.responses.create(
            model=chatbot.model,
            input=temp_context,  # user/assistant ì—­í•  í¬í•¨ëœ list êµ¬ì¡°
            top_p=1,
            stream=True,
            text={
                "format": {
                    "type": "text"  # ë˜ëŠ” "json_object" ë“± (Structured Output ì‚¬ìš© ì‹œ)
                }
            }
                )
              
            # loading = True
            for event in stream:
                        match event.type:
                            # case "response.created":
                            #     print("[ğŸ¤– ì‘ë‹µ ìƒì„± ì‹œì‘]")
                            #     loading = True
                            #     # ë¡œë”© ì• ë‹ˆë©”ì´ì…˜ìš© ëŒ€ê¸° ì‹œì‘
                            #     yield "â³ GPTê°€ ì‘ë‹µì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤..."
                            #     await asyncio.sleep(0)
                            case "response.output_text.delta":
                                # if loading:
                                #     print("\n[ğŸ’¬ ì‘ë‹µ ì‹œì‘ë¨ â†“]")

                                #     loading = False
                                # ê¸€ì ë‹¨ìœ„ ì¶œë ¥
                                yield f"{event.delta}"
                                await asyncio.sleep(0)
                            

                            # case "response.in_progress":
                            #     print("[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]")
                            #     yield "[ğŸŒ€ ì‘ë‹µ ìƒì„± ì¤‘...]"
                            #     yield "\n"

                            # case "response.output_item.added":
                            #     if getattr(event.item, "type", None) == "reasoning":
                            #         yield "[ğŸ§  GPTê°€ ì¶”ë¡ ì„ ì‹œì‘í•©ë‹ˆë‹¤...]"
                            #         yield "\n"
                            #     elif getattr(event.item, "type", None) == "message":
                            #         yield "[ğŸ“© ë©”ì‹œì§€ ì•„ì´í…œ ì¶”ê°€ë¨]"
                            #         yield "\n"
                            # #ResponseOutputItemDoneEventëŠ” ìš°ë¦¬ê°€ case "response.output_item.done"ì—ì„œ ì¡ì•„ì•¼ í•´
                            case "response.output_item.done":
                                item = event.item
                                if item.type == "message" and item.role == "assistant":
                                    for part in item.content:
                                        if getattr(part, "type", None) == "output_text":
                                            completed_text= part.text
                            # case "response.completed":
                            #     yield "\n"
                            #     #print(f"\nğŸ“¦ ìµœì¢… ì „ì²´ ì¶œë ¥: \n{completed_text}")
                            # case "response.failed":
                            #     print("âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨")
                            #     yield "âŒ ì‘ë‹µ ìƒì„± ì‹¤íŒ¨"
                            # case "error":
                            #     print("âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!")
                            #     yield "âš ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì—ëŸ¬ ë°œìƒ!"
                            # case _:
                            #     yield "\n"
                            #     yield f"[ğŸ“¬ ê¸°íƒ€ ì´ë²¤íŠ¸ ê°ì§€: {event.type}]"
        except Exception as e:
            yield f"\nStream Error: {str(e)}"
        finally:
            # ìŠ¤íŠ¸ë¦¬ë°ì´ ëë‚˜ë©´ ìµœì¢… ì‘ë‹µì„ ì›ë³¸ ë¬¸ë§¥ì—ë§Œ ë°˜ì˜í•˜ê³  ì„ì‹œë¬¸ë§¥ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                            # ê¸°ì¡´ clean ë°©ì‹ ìœ ì§€
            chatbot.add_response_stream( completed_text)
            
                        # ìµœì¢… ì‘ë‹µì„ ì›ë³¸ ë¬¸ë§¥ì— ì €ì¥
    # 5) í•¨ìˆ˜ í˜¸ì¶œì´ ìˆì„ ë•ŒëŠ” ìœ„ì˜ generate_with_tool()ë¥¼ ì‚¬ìš©
    return StreamingResponse(generate_with_tool(), media_type="text/plain")